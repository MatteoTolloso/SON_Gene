from builtins import dict
from os import terminal_size
import sys
import re
from textwrap import indent
import networkx as nx
from itertools import combinations
import matplotlib.pyplot as plt
from tabulate import tabulate
import requests
import json


"""
Dataset ottenuto dalla query:

(SON[Title/Abstract] AND ("DNA-Binding Proteins"[Mesh] OR "RNA-Binding Proteins"[Mesh]) AND "SON protein, human"[nm]) OR "SON gene"[All Fields] OR "SON protein"[All Fields]\
OR NREBP[Title/Abstract]\
OR ("DBP-5"[Title/Abstract] AND "DNA-Binding Proteins"[MeSH])\
OR "NRE-Binding Protein"[Title/Abstract]\
OR KIAA1019[Title/Abstract]\
OR C21orf50[Title/Abstract]\
OR (SON3[Title/Abstract] AND "DNA-Binding Proteins"[MeSH])\
OR DBP5[Title/Abstract]'
"""


def build_dataset(pubMedFilePath : str , pathToSave : str) -> dict:
    
    articlesStr : list[str] = []
    content = ''
    with open(pubMedFilePath,'r') as file:
        content = file.read()
    
    articlesStr = list(map(lambda x: x.replace('\n      ', ' '), content.split("\n\n")))

    dict = {}
    for art in articlesStr:

        # PubMed ID
        match = re.search('^PMID- (.*)$', art, re.MULTILINE)
        if match is None:
            continue
        pmid = match.group(1)

        # Title
        match = re.search('^TI  - (.*)$', art, re.MULTILINE)
        if match is None:
            continue
        ti = match.group(1)

        # Abstract
        match = re.search('^AB  - (.*)$', art, re.MULTILINE)
        if match is None:
            continue
        ab = match.group(1)

        # NLM Medical Subject Headings (MeSH) controlled vocabulary
        mh = re.findall('^MH  - (.*)$', art, re.MULTILINE)
        
        # Includes chemical, protocol or disease terms. May also a number assigned by the Enzyme Commission or by the Chemical Abstracts Service.
        rn = re.findall('^RN  - (.*)$', art, re.MULTILINE)

        # Non-MeSH subject terms (keywords) either assigned by an organization identified by the Other Term Owner, or generated by the author and submitted by the publisher
        ot = re.findall('^OT  - (.*)$', art, re.MULTILINE)

        dict[pmid] = {'Title' : ti, 'Abstract' : ab, 'MeSH' : mh, 'RNnumber': rn, 'OtherTerm': ot, 'bioBERT_entities': [] }

    base_url : str = "https://bern.korea.ac.kr/pubmed/"
    i = 0
    for id in dict.keys():
        i+=1
        print('processing article', i, 'of', len(dict.keys()))
        url = base_url + id
        response = requests.get(url, verify=False).json()
        denotations = response[0].get('denotations')
        for den in denotations:
            begin = den.get('span').get('begin')
            end = den.get('span').get('end')
            entity_type = den.get('obj')
            entity_name = response[0].get('text')[begin:end]
            dict[id].get('bioBERT_entities').append((entity_name, entity_type))
        
    dataset_string_json = json.dumps(dict, indent=4)

    with open(pathToSave, 'w') as file:
        file.write(dataset_string_json)
        
        
    return dict

def extract_mesh(articles : dict) -> set:
    mesh_terms = set()
    for art in articles.values():
        local_mesh = art.get('MeSH')
        if local_mesh is not None:
            mesh_terms.update(local_mesh)
    return mesh_terms


def build_cooccurrences_graph(  articles : dict, 
                                mh = True, 
                                rn = True, 
                                ot = True, 
                                bbent=True, 
                                check_tags=[], 
                                thesaurus={},
                                bbent_types = ['disease', 'gene', 'drug', 'species']) -> nx.Graph:
    
    graph = nx.Graph()

    for art in articles.values():
        terms = []
        
        # extract the types of terms required
        if mh:
            terms += art.get('MeSH')
        if rn:
            terms += art.get('RNnumber')
        if ot:
            terms += art.get('OtherTerm')
        if bbent:
            ents = art.get('bioBERT_entities') #list of touples (name, type)
            for ent in ents:
                if ent[1] in bbent_types:
                    terms.append(ent[0])

        
        #remove check tags
        terms = list(filter(lambda x: x not in check_tags, terms))
        
        # merge synonyms
        for key in thesaurus.keys(): # for all key in thesaurus 
            for pos, term in enumerate(terms): # for all terms in this article
                for syn in thesaurus[key]:  # for all synonyms
                    if syn in term: #if the target is contained in the term
                        terms[pos] = key  #replace the term with the synonymous
                        break
        
        terms = list(set(terms))

        if not terms:
            continue
        
        for a, b in list(combinations(terms, 2)):
            if not graph.has_node(a):
                graph.add_node(a, weight=1)
            else:
                graph.nodes[a]['weight'] += 1
            
            if not graph.has_node(b):
                graph.add_node(b, weight=1)
            else:
                graph.nodes[b]['weight'] += 1
            
            if graph.has_edge(a, b):
                graph[a][b]['weight'] += 1
            else:
                graph.add_edge(a, b, weight=1)
    
    return graph

def draw(graph: nx.Graph):
   
    #layout
    pos = nx.spring_layout(graph, pos= {'SON': [0, 0]}, fixed=['SON'], seed=1)
    #pos = nx.spectral_layout(graph, weight='weight')

    #edges
    edgewidth = [ (graph[u][v]['weight'] * 0.8) for u, v in graph.edges()]
    #nx.draw_networkx_edges(graph, pos, alpha=0.3, width=edgewidth, edge_color="m", edgelist=graph.edges(nbunch='SON'))
    nx.draw_networkx_edges(graph, pos, alpha=0.3, width=edgewidth, edge_color="m")
    
    #nodes
    nodesize = [ (graph.nodes[v]['weight']* 2) for v in graph.nodes()]
    nx.draw_networkx_nodes(graph, pos, node_size=nodesize, node_color="b", alpha=0.9)
    label_options = {"ec": "k", "fc": "white", "alpha": 0.7}
    nx.draw_networkx_labels(graph, pos, font_size=12, bbox=label_options)


    plt.axis("off")
    plt.show()
   

def main():

    if(len(sys.argv) < 2):
        print("Usage: $ python3", sys.argv[0], "<dataset_path>")
        return
    path = sys.argv[1]

    content = ''
    with open(path, 'r') as file:
        content = file.read()
    
    #TODO dataset rebuilder
    if content == '':
        print('No content, please rebuild dataset')
        return
    
    articles : dict = json.loads(content)
    
    print('Numero di articoli: ', len(articles.keys()))

    #mesh_terms : set = extract_mesh(articles)
    #print('Numero di MeSH diversi: ', len(mesh_terms))

    ct = ['Humans', 'Animals']
    th = {'SON' : [ 'SON', 'NREBP', 'DBP-5', 'NRE-Binding Protein', 'KIAA1019', 'C21orf50', 'DBP5', 'SON3', 'Dbp5']}

    cooccurrences_graph = build_cooccurrences_graph(articles, check_tags=ct, thesaurus=th, rn=False, mh=False, ot=False)
    print('Grafo delle co-occorrenze:\n\tNodi: ', len(cooccurrences_graph.nodes), '\n\tArchi: ', len(cooccurrences_graph.edges))

    cooccurrences_list = list(cooccurrences_graph.edges.data('weight'))
    cooccurrences_list.sort(key = lambda x:x[2], reverse=True)
    with open('cooccurrences.txt', 'w') as file:
            file.write(tabulate(cooccurrences_list,  headers=['Entity', 'Entity', 'Number of cooccurrences'],  tablefmt='orgtbl'))

    print('Il grafo ha: ', nx.number_connected_components(cooccurrences_graph), ' componenti connesse')

    print('Coefficiente di clustering: ', nx.average_clustering(cooccurrences_graph))

    nodes = list(cooccurrences_graph.nodes.data('weight'))
    nodes.sort(key = lambda x:x[1], reverse=True)
    with open('nodes.txt', 'w') as file:
            file.write(tabulate(nodes, headers=['Entity', 'Number of occurrences'], tablefmt='orgtbl'))

    main_nodes = []
    for i in range(100):
        main_nodes.append(nodes[i][0])
    
    main_graph = cooccurrences_graph.subgraph(main_nodes)

    draw(main_graph)


if __name__ == "__main__":
    main()
    
    
    








